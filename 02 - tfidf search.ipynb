{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ('headers','quotes','footers')\n",
    "newsgroups = fetch_20newsgroups(remove=remove)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `TfidfVectorizer`\n",
    "- Only uses unigrams\n",
    "- Log-scales the inverse document frequency as smooths it (adding 1)\n",
    "- Normalizes each row with $L_2$ normalization (the sum of squares is 1), so that the dot product of two rows is equal to their cosine similarity \n",
    "\n",
    "It's not that important to remove stopwords, they are penalized anyway by the `idf` part (stopwords appear in almost all documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(newsgroups.data)\n",
    "corpus_vec = vectorizer.transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 1103627 stored elements and shape (11314, 101631)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a query, a simple way to find the most relevant documents in the corpus is to vectorize the query with the same vectorizer, and find the documents with the highest cosine similarity. \n",
    "\n",
    "The 20 newsgroups dataset is not particularly large, so we probably won't gain much by using the `HashingVectorizer`, but this would be a good application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"cat\"] # has to be an iterable over documents\n",
    "\n",
    "query_vec = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 1 stored elements and shape (1, 101631)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = corpus_vec.dot(query_vec.T).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices in the array with the 5 highest similarity scores\n",
    "top_idx = np.argpartition(similarity_scores,-5)[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main weakness of TF-IDF is the lack of context awareness, here the query \"cat\" matches both the animal and the linux command `cat`. This will be fixed by more modern approaches, such BERT. Word2Vec still has the same problem (`cat` with either meaning goes to the same embedding), but has the advantage of enconding some semantic similarity: if we train Word2Vec on a large corpus containing both discussion of domestic animals and computers, we expect the embedding for `cat` to be both close to other animals and other Linux commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = {i:name for i,name in enumerate(newsgroups.target_names)}\n",
    "readable_labels = np.array([categories_dict[label] for label in newsgroups.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.3939521519157273\n",
      "Group:  sci.electronics\n",
      "Sample:\n",
      "\n",
      "We use them as Christmas tree decorations, the cat doesn't eat these.\n",
      "\n",
      "-- \n",
      "\n",
      "Similarity:  0.31950344372186057\n",
      "Group:  comp.os.ms-windows.misc\n",
      "Sample:\n",
      "\n",
      "Because you are uptight?\n",
      "\n",
      "Many computer-literate people see advantages in each system.\n",
      "\n",
      "You act like a Mac ate your cat.\n",
      "\n",
      "Similarity:  0.2588604371075505\n",
      "Group:  rec.motorcycles\n",
      "Sample:\n",
      "\n",
      "\tThere should be no worries about the trans.\n",
      "\n",
      "\n",
      "\tDoes this count?\n",
      "\n",
      "$ cat dod.faq | mailx -s \"HAHAHHA\" jburnside@ll.mit.edu (waiting to press\n",
      "\t\t\t\t\t\t\t return...)\n",
      "\n",
      "Later,\n",
      "\n",
      "Similarity:  0.23204008172780952\n",
      "Group:  rec.sport.hockey\n",
      "Sample:\n",
      "\n",
      "It would seem logical that the mask is Potvins. His nickname is \"The Cat\", \n",
      "which would go a long ways towards explaining the panther. \n",
      "\n",
      "Of course, it could be an old story and the mask is Fuhrs, too.....\n",
      "\n",
      "Similarity:  0.21892456660595316\n",
      "Group:  misc.forsale\n",
      "Sample:\n",
      "For Sale:\n",
      "\n",
      "1982 - 16' Hobie Cat Special, very good condition with\n",
      "trailer, catbox, righting system, many extras.  Boat\n",
      "is currently garaged in Natick MA, 25 miles east of\n",
      "Boston.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in reversed(top_idx):\n",
    "    print(\"Similarity: \",similarity_scores[idx])\n",
    "    print(\"Group: \",readable_labels[idx])\n",
    "    print(\"Sample:\")\n",
    "    print(newsgroups.data[idx][:500],end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
